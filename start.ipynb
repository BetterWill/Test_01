{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from keras.layers import Input, Dropout\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_attention_layer import GraphAttention\n",
    "from  untils import load_data,preprocess_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "A, X, Y_train, Y_val, Y_test, idx_train, idx_val, idx_test = load_data('cora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X的shape: (2708, 1433)\n",
      "A的shape: (2708, 2708)\n",
      "Y_train的shape: (2708, 7)\n"
     ]
    }
   ],
   "source": [
    "print('X的shape:',X.shape)\n",
    "print('A的shape:',A.shape)\n",
    "print('Y_train的shape:',Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "N = X.shape[0]                # Number of nodes in the graph\n",
    "F = X.shape[1]                # Original feature dimension\n",
    "n_classes = Y_train.shape[1]  #标签的维度\n",
    "F_ = 8                        # Output size of first GraphAttention layer\n",
    "n_attn_heads = 8              # Number of attention heads  即K值\n",
    "dropout_rate = 0.6            # Dropout rate (between and inside GAT layers)\n",
    "l2_reg = 5e-4/2\n",
    "learning_rate = 5e-3\n",
    "epochs = 1000\n",
    "es_patience = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对行进行规范化\n",
    "X= preprocess_features(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2708, 1433)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = A +np.eye(A.shape[0])  #加上自连接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 1433)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 1433)         0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 2708)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_attention_3 (GraphAttenti (None, 64)           91904       dropout_21[0][0]                 \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 64)           0           graph_attention_3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "graph_attention_4 (GraphAttenti (None, 7)            469         dropout_38[0][0]                 \n",
      "                                                                 input_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 92,373\n",
      "Trainable params: 92,373\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## 定义模型输入\n",
    "X_in = Input(shape=(F,))\n",
    "A_in = Input(shape=(N,))\n",
    "\n",
    "dropout1 = Dropout(dropout_rate)(X_in)\n",
    "\n",
    "#第一次注意力层\n",
    "graph_attention_1 = GraphAttention(F_,\n",
    "                                  attn_heads=n_attn_heads,\n",
    "                                  dropout_rate=dropout_rate,\n",
    "                                  activation='elu',\n",
    "                                  kernel_regularizer=l2(l2_reg),\n",
    "                                  attn_kernel_regularizer=l2(l2_reg))([dropout1, A_in])\n",
    "\n",
    "\n",
    "dropout2 = Dropout(dropout_rate)(graph_attention_1)\n",
    "\n",
    "#第二次注意力层\n",
    "graph_attention_2 = GraphAttention(n_classes,\n",
    "                                  attn_heads=1,\n",
    "                                  attn_heads_reduction='average',\n",
    "                                  dropout_rate=dropout_rate,\n",
    "                                  activation='softmax',\n",
    "                                  kernel_regularizer=l2(l2_reg),\n",
    "                                  attn_kernel_regularizer=l2(l2_reg))([dropout2, A_in])\n",
    "\n",
    "model = Model(inputs=[X_in, A_in], outputs=graph_attention_2)\n",
    "\n",
    "optimizer = Adam(lr=learning_rate)\n",
    "model.compile(optimizer = optimizer,\n",
    "             loss='categorical_crossentropy',\n",
    "             weighted_metrics=['acc'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Callbacks\n",
    "es_callback = EarlyStopping(monitor='val_weighted_acc', patience=es_patience)\n",
    "# tb_callback = TensorBoard(batch_size=N)\n",
    "mc_callback = ModelCheckpoint('logs/best_model.h5',\n",
    "                              monitor='val_weighted_acc',\n",
    "                              save_best_only=True,\n",
    "                              save_weights_only=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2708 samples, validate on 2708 samples\n",
      "Epoch 1/1000\n",
      "2708/2708 [==============================] - 2s 657us/step - loss: 0.1436 - acc: 0.1786 - val_loss: 0.3967 - val_acc: 0.1420\n",
      "Epoch 2/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\learn software\\anaconda3\\lib\\site-packages\\keras\\callbacks\\callbacks.py:846: RuntimeWarning: Early stopping conditioned on metric `val_weighted_acc` which is not available. Available metrics are: val_loss,val_acc,loss,acc\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n",
      "D:\\learn software\\anaconda3\\lib\\site-packages\\keras\\callbacks\\callbacks.py:707: RuntimeWarning: Can save best model only with val_weighted_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2708/2708 [==============================] - 0s 157us/step - loss: 0.1379 - acc: 0.1071 - val_loss: 0.3913 - val_acc: 0.1780\n",
      "Epoch 3/1000\n",
      "2708/2708 [==============================] - 0s 155us/step - loss: 0.1324 - acc: 0.2000 - val_loss: 0.3865 - val_acc: 0.1420\n",
      "Epoch 4/1000\n",
      "2708/2708 [==============================] - 0s 160us/step - loss: 0.1281 - acc: 0.1571 - val_loss: 0.3823 - val_acc: 0.1600\n",
      "Epoch 5/1000\n",
      "2708/2708 [==============================] - 0s 172us/step - loss: 0.1240 - acc: 0.1929 - val_loss: 0.3786 - val_acc: 0.1440\n",
      "Epoch 6/1000\n",
      "2708/2708 [==============================] - 0s 157us/step - loss: 0.1205 - acc: 0.1929 - val_loss: 0.3755 - val_acc: 0.1480\n",
      "Epoch 7/1000\n",
      "2708/2708 [==============================] - 0s 169us/step - loss: 0.1182 - acc: 0.1571 - val_loss: 0.3731 - val_acc: 0.1520\n",
      "Epoch 8/1000\n",
      "2708/2708 [==============================] - 0s 156us/step - loss: 0.1158 - acc: 0.1857 - val_loss: 0.3711 - val_acc: 0.1640\n",
      "Epoch 9/1000\n",
      "2708/2708 [==============================] - 0s 159us/step - loss: 0.1138 - acc: 0.1571 - val_loss: 0.3696 - val_acc: 0.2740\n",
      "Epoch 10/1000\n",
      "2708/2708 [==============================] - 0s 163us/step - loss: 0.1125 - acc: 0.1929 - val_loss: 0.3685 - val_acc: 0.3100\n",
      "Epoch 11/1000\n",
      "2708/2708 [==============================] - 0s 162us/step - loss: 0.1119 - acc: 0.1786 - val_loss: 0.3678 - val_acc: 0.3180\n",
      "Epoch 12/1000\n",
      "2708/2708 [==============================] - 0s 159us/step - loss: 0.1109 - acc: 0.1929 - val_loss: 0.3674 - val_acc: 0.2960\n",
      "Epoch 13/1000\n",
      "2708/2708 [==============================] - 0s 155us/step - loss: 0.1106 - acc: 0.1929 - val_loss: 0.3673 - val_acc: 0.2600\n",
      "Epoch 14/1000\n",
      "2708/2708 [==============================] - 0s 168us/step - loss: 0.1096 - acc: 0.3071 - val_loss: 0.3673 - val_acc: 0.3020\n",
      "Epoch 15/1000\n",
      "2708/2708 [==============================] - 0s 168us/step - loss: 0.1095 - acc: 0.3143 - val_loss: 0.3674 - val_acc: 0.3280\n",
      "Epoch 16/1000\n",
      "2708/2708 [==============================] - 0s 161us/step - loss: 0.1098 - acc: 0.2571 - val_loss: 0.3675 - val_acc: 0.3640\n",
      "Epoch 17/1000\n",
      "2708/2708 [==============================] - 0s 172us/step - loss: 0.1089 - acc: 0.3643 - val_loss: 0.3676 - val_acc: 0.4400\n",
      "Epoch 18/1000\n",
      "2708/2708 [==============================] - 0s 161us/step - loss: 0.1094 - acc: 0.2786 - val_loss: 0.3676 - val_acc: 0.4680\n",
      "Epoch 19/1000\n",
      "2708/2708 [==============================] - 0s 155us/step - loss: 0.1098 - acc: 0.2500 - val_loss: 0.3677 - val_acc: 0.4240\n",
      "Epoch 20/1000\n",
      "2708/2708 [==============================] - 0s 160us/step - loss: 0.1098 - acc: 0.1714 - val_loss: 0.3677 - val_acc: 0.3900\n",
      "Epoch 21/1000\n",
      "2708/2708 [==============================] - 0s 154us/step - loss: 0.1090 - acc: 0.2143 - val_loss: 0.3676 - val_acc: 0.3560\n",
      "Epoch 22/1000\n",
      "2708/2708 [==============================] - 0s 161us/step - loss: 0.1085 - acc: 0.3071 - val_loss: 0.3675 - val_acc: 0.2980\n",
      "Epoch 23/1000\n",
      "2708/2708 [==============================] - 0s 168us/step - loss: 0.1087 - acc: 0.2286 - val_loss: 0.3673 - val_acc: 0.2600\n",
      "Epoch 24/1000\n",
      "2708/2708 [==============================] - 0s 163us/step - loss: 0.1084 - acc: 0.2286 - val_loss: 0.3670 - val_acc: 0.2400\n",
      "Epoch 25/1000\n",
      "2708/2708 [==============================] - 0s 162us/step - loss: 0.1083 - acc: 0.2857 - val_loss: 0.3666 - val_acc: 0.2460\n",
      "Epoch 26/1000\n",
      "2708/2708 [==============================] - 0s 164us/step - loss: 0.1081 - acc: 0.2000 - val_loss: 0.3662 - val_acc: 0.2580\n",
      "Epoch 27/1000\n",
      "2708/2708 [==============================] - 0s 169us/step - loss: 0.1074 - acc: 0.3000 - val_loss: 0.3658 - val_acc: 0.2400\n",
      "Epoch 28/1000\n",
      "2708/2708 [==============================] - 0s 163us/step - loss: 0.1073 - acc: 0.2571 - val_loss: 0.3654 - val_acc: 0.2100\n",
      "Epoch 29/1000\n",
      "2708/2708 [==============================] - 0s 160us/step - loss: 0.1071 - acc: 0.3000 - val_loss: 0.3650 - val_acc: 0.1840\n",
      "Epoch 30/1000\n",
      "2708/2708 [==============================] - 0s 158us/step - loss: 0.1072 - acc: 0.2214 - val_loss: 0.3646 - val_acc: 0.1880\n",
      "Epoch 31/1000\n",
      "2708/2708 [==============================] - 0s 160us/step - loss: 0.1072 - acc: 0.2071 - val_loss: 0.3642 - val_acc: 0.1920\n",
      "Epoch 32/1000\n",
      "2708/2708 [==============================] - 0s 161us/step - loss: 0.1058 - acc: 0.2929 - val_loss: 0.3639 - val_acc: 0.2420\n",
      "Epoch 33/1000\n",
      "2708/2708 [==============================] - 0s 173us/step - loss: 0.1067 - acc: 0.2286 - val_loss: 0.3636 - val_acc: 0.3800\n",
      "Epoch 34/1000\n",
      "2708/2708 [==============================] - 0s 165us/step - loss: 0.1062 - acc: 0.2429 - val_loss: 0.3634 - val_acc: 0.4440\n",
      "Epoch 35/1000\n",
      "2708/2708 [==============================] - 0s 167us/step - loss: 0.1061 - acc: 0.2429 - val_loss: 0.3631 - val_acc: 0.4900\n",
      "Epoch 36/1000\n",
      "2708/2708 [==============================] - 0s 169us/step - loss: 0.1059 - acc: 0.2500 - val_loss: 0.3629 - val_acc: 0.4900\n",
      "Epoch 37/1000\n",
      "2708/2708 [==============================] - 0s 167us/step - loss: 0.1060 - acc: 0.2500 - val_loss: 0.3626 - val_acc: 0.5000\n",
      "Epoch 38/1000\n",
      "2708/2708 [==============================] - 0s 168us/step - loss: 0.1060 - acc: 0.2143 - val_loss: 0.3624 - val_acc: 0.4860\n",
      "Epoch 39/1000\n",
      "2708/2708 [==============================] - 0s 163us/step - loss: 0.1053 - acc: 0.2429 - val_loss: 0.3623 - val_acc: 0.4840\n",
      "Epoch 40/1000\n",
      "2708/2708 [==============================] - 0s 165us/step - loss: 0.1061 - acc: 0.1857 - val_loss: 0.3621 - val_acc: 0.5100\n",
      "Epoch 41/1000\n",
      "2708/2708 [==============================] - 0s 169us/step - loss: 0.1056 - acc: 0.2071 - val_loss: 0.3620 - val_acc: 0.5560\n",
      "Epoch 42/1000\n",
      "2708/2708 [==============================] - 0s 170us/step - loss: 0.1048 - acc: 0.2571 - val_loss: 0.3619 - val_acc: 0.5680\n",
      "Epoch 43/1000\n",
      "2708/2708 [==============================] - 0s 168us/step - loss: 0.1052 - acc: 0.2000 - val_loss: 0.3618 - val_acc: 0.5100\n",
      "Epoch 44/1000\n",
      "2708/2708 [==============================] - 0s 174us/step - loss: 0.1048 - acc: 0.2286 - val_loss: 0.3618 - val_acc: 0.4060\n",
      "Epoch 45/1000\n",
      "2708/2708 [==============================] - 0s 171us/step - loss: 0.1047 - acc: 0.2786 - val_loss: 0.3617 - val_acc: 0.3800\n",
      "Epoch 46/1000\n",
      "2708/2708 [==============================] - 0s 177us/step - loss: 0.1050 - acc: 0.2143 - val_loss: 0.3616 - val_acc: 0.3620\n",
      "Epoch 47/1000\n",
      "2708/2708 [==============================] - 0s 172us/step - loss: 0.1049 - acc: 0.2143 - val_loss: 0.3615 - val_acc: 0.3460\n",
      "Epoch 48/1000\n",
      "2708/2708 [==============================] - 0s 179us/step - loss: 0.1044 - acc: 0.1857 - val_loss: 0.3615 - val_acc: 0.3420\n",
      "Epoch 49/1000\n",
      "2708/2708 [==============================] - 0s 179us/step - loss: 0.1044 - acc: 0.2857 - val_loss: 0.3615 - val_acc: 0.3340\n",
      "Epoch 50/1000\n",
      "2708/2708 [==============================] - 0s 176us/step - loss: 0.1046 - acc: 0.2643 - val_loss: 0.3616 - val_acc: 0.3400\n",
      "Epoch 51/1000\n",
      "2708/2708 [==============================] - 0s 164us/step - loss: 0.1045 - acc: 0.2286 - val_loss: 0.3615 - val_acc: 0.3720\n",
      "Epoch 52/1000\n",
      "2708/2708 [==============================] - 0s 161us/step - loss: 0.1042 - acc: 0.2786 - val_loss: 0.3615 - val_acc: 0.4040\n",
      "Epoch 53/1000\n",
      "2708/2708 [==============================] - 0s 162us/step - loss: 0.1039 - acc: 0.2643 - val_loss: 0.3615 - val_acc: 0.3660\n",
      "Epoch 54/1000\n",
      "2708/2708 [==============================] - 0s 166us/step - loss: 0.1037 - acc: 0.3000 - val_loss: 0.3615 - val_acc: 0.3420\n",
      "Epoch 55/1000\n",
      "2708/2708 [==============================] - 0s 171us/step - loss: 0.1041 - acc: 0.2071 - val_loss: 0.3614 - val_acc: 0.3320\n",
      "Epoch 56/1000\n",
      "2708/2708 [==============================] - 0s 176us/step - loss: 0.1038 - acc: 0.2429 - val_loss: 0.3614 - val_acc: 0.3220\n",
      "Epoch 57/1000\n",
      "2708/2708 [==============================] - 0s 170us/step - loss: 0.1038 - acc: 0.2214 - val_loss: 0.3614 - val_acc: 0.2740\n",
      "Epoch 58/1000\n",
      "2708/2708 [==============================] - 0s 173us/step - loss: 0.1041 - acc: 0.1571 - val_loss: 0.3613 - val_acc: 0.2260\n",
      "Epoch 59/1000\n",
      "2708/2708 [==============================] - 0s 174us/step - loss: 0.1039 - acc: 0.1857 - val_loss: 0.3613 - val_acc: 0.1820\n",
      "Epoch 60/1000\n",
      "2708/2708 [==============================] - 0s 172us/step - loss: 0.1038 - acc: 0.2500 - val_loss: 0.3613 - val_acc: 0.1600\n",
      "Epoch 61/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2708/2708 [==============================] - 0s 169us/step - loss: 0.1035 - acc: 0.1857 - val_loss: 0.3613 - val_acc: 0.1480\n",
      "Epoch 62/1000\n",
      "2708/2708 [==============================] - 0s 168us/step - loss: 0.1034 - acc: 0.2286 - val_loss: 0.3613 - val_acc: 0.1500\n",
      "Epoch 63/1000\n",
      "2708/2708 [==============================] - 0s 166us/step - loss: 0.1034 - acc: 0.2429 - val_loss: 0.3613 - val_acc: 0.2020\n",
      "Epoch 64/1000\n",
      "2708/2708 [==============================] - 0s 166us/step - loss: 0.1033 - acc: 0.2429 - val_loss: 0.3613 - val_acc: 0.4020\n",
      "Epoch 65/1000\n",
      "2708/2708 [==============================] - 0s 168us/step - loss: 0.1031 - acc: 0.2786 - val_loss: 0.3613 - val_acc: 0.4180\n",
      "Epoch 66/1000\n",
      "2708/2708 [==============================] - 0s 171us/step - loss: 0.1034 - acc: 0.1857 - val_loss: 0.3613 - val_acc: 0.3020\n",
      "Epoch 67/1000\n",
      "2708/2708 [==============================] - 0s 170us/step - loss: 0.1032 - acc: 0.2286 - val_loss: 0.3613 - val_acc: 0.1320\n",
      "Epoch 68/1000\n",
      "2708/2708 [==============================] - 0s 170us/step - loss: 0.1033 - acc: 0.1786 - val_loss: 0.3613 - val_acc: 0.0980\n",
      "Epoch 69/1000\n",
      "2708/2708 [==============================] - 0s 171us/step - loss: 0.1029 - acc: 0.2643 - val_loss: 0.3613 - val_acc: 0.0860\n",
      "Epoch 70/1000\n",
      "2708/2708 [==============================] - 0s 171us/step - loss: 0.1028 - acc: 0.2071 - val_loss: 0.3612 - val_acc: 0.0860\n",
      "Epoch 71/1000\n",
      "2708/2708 [==============================] - 0s 172us/step - loss: 0.1028 - acc: 0.2214 - val_loss: 0.3612 - val_acc: 0.0880\n",
      "Epoch 72/1000\n",
      "2708/2708 [==============================] - 0s 169us/step - loss: 0.1029 - acc: 0.2143 - val_loss: 0.3611 - val_acc: 0.0920\n",
      "Epoch 73/1000\n",
      "2708/2708 [==============================] - 0s 165us/step - loss: 0.1028 - acc: 0.2071 - val_loss: 0.3611 - val_acc: 0.1080\n",
      "Epoch 74/1000\n",
      "2708/2708 [==============================] - 0s 166us/step - loss: 0.1030 - acc: 0.2000 - val_loss: 0.3610 - val_acc: 0.1380\n",
      "Epoch 75/1000\n",
      "2708/2708 [==============================] - 0s 165us/step - loss: 0.1030 - acc: 0.2000 - val_loss: 0.3608 - val_acc: 0.2120\n",
      "Epoch 76/1000\n",
      "2708/2708 [==============================] - 0s 166us/step - loss: 0.1027 - acc: 0.2143 - val_loss: 0.3608 - val_acc: 0.2760\n",
      "Epoch 77/1000\n",
      "2708/2708 [==============================] - 0s 170us/step - loss: 0.1025 - acc: 0.2714 - val_loss: 0.3607 - val_acc: 0.3520\n",
      "Epoch 78/1000\n",
      "2708/2708 [==============================] - 0s 168us/step - loss: 0.1024 - acc: 0.2714 - val_loss: 0.3606 - val_acc: 0.3640\n",
      "Epoch 79/1000\n",
      "2708/2708 [==============================] - 0s 171us/step - loss: 0.1027 - acc: 0.1857 - val_loss: 0.3605 - val_acc: 0.3780\n",
      "Epoch 80/1000\n",
      "2708/2708 [==============================] - 0s 170us/step - loss: 0.1026 - acc: 0.1929 - val_loss: 0.3605 - val_acc: 0.3900\n",
      "Epoch 81/1000\n",
      "2708/2708 [==============================] - 0s 170us/step - loss: 0.1028 - acc: 0.1286 - val_loss: 0.3605 - val_acc: 0.4800\n",
      "Epoch 82/1000\n",
      "2708/2708 [==============================] - 0s 171us/step - loss: 0.1023 - acc: 0.2786 - val_loss: 0.3606 - val_acc: 0.4620\n",
      "Epoch 83/1000\n",
      "2708/2708 [==============================] - 0s 168us/step - loss: 0.1027 - acc: 0.1714 - val_loss: 0.3606 - val_acc: 0.3340\n",
      "Epoch 84/1000\n",
      "2708/2708 [==============================] - 0s 167us/step - loss: 0.1023 - acc: 0.2714 - val_loss: 0.3607 - val_acc: 0.2540\n",
      "Epoch 85/1000\n",
      "2708/2708 [==============================] - 0s 166us/step - loss: 0.1023 - acc: 0.2000 - val_loss: 0.3607 - val_acc: 0.2060\n",
      "Epoch 86/1000\n",
      "2708/2708 [==============================] - 0s 167us/step - loss: 0.1022 - acc: 0.2286 - val_loss: 0.3607 - val_acc: 0.1880\n",
      "Epoch 87/1000\n",
      "2708/2708 [==============================] - 0s 166us/step - loss: 0.1025 - acc: 0.2000 - val_loss: 0.3606 - val_acc: 0.1680\n",
      "Epoch 88/1000\n",
      "2708/2708 [==============================] - 0s 170us/step - loss: 0.1021 - acc: 0.1857 - val_loss: 0.3605 - val_acc: 0.1520\n",
      "Epoch 89/1000\n",
      "2708/2708 [==============================] - 0s 170us/step - loss: 0.1020 - acc: 0.2643 - val_loss: 0.3604 - val_acc: 0.1420\n",
      "Epoch 90/1000\n",
      "2708/2708 [==============================] - 0s 171us/step - loss: 0.1022 - acc: 0.2214 - val_loss: 0.3603 - val_acc: 0.1240\n",
      "Epoch 91/1000\n",
      "2708/2708 [==============================] - 0s 170us/step - loss: 0.1021 - acc: 0.1929 - val_loss: 0.3602 - val_acc: 0.1240\n",
      "Epoch 92/1000\n",
      "2708/2708 [==============================] - 0s 171us/step - loss: 0.1020 - acc: 0.2500 - val_loss: 0.3602 - val_acc: 0.1500\n",
      "Epoch 93/1000\n",
      "2708/2708 [==============================] - 0s 171us/step - loss: 0.1021 - acc: 0.2143 - val_loss: 0.3601 - val_acc: 0.1740\n",
      "Epoch 94/1000\n",
      "2708/2708 [==============================] - 0s 166us/step - loss: 0.1019 - acc: 0.2429 - val_loss: 0.3601 - val_acc: 0.2400\n",
      "Epoch 95/1000\n",
      "2708/2708 [==============================] - 0s 168us/step - loss: 0.1021 - acc: 0.2357 - val_loss: 0.3601 - val_acc: 0.3820\n",
      "Epoch 96/1000\n",
      "2708/2708 [==============================] - 0s 166us/step - loss: 0.1018 - acc: 0.2500 - val_loss: 0.3600 - val_acc: 0.4000\n",
      "Epoch 97/1000\n",
      "2708/2708 [==============================] - 0s 168us/step - loss: 0.1018 - acc: 0.1929 - val_loss: 0.3600 - val_acc: 0.2640\n",
      "Epoch 98/1000\n",
      "2708/2708 [==============================] - 0s 169us/step - loss: 0.1017 - acc: 0.2643 - val_loss: 0.3601 - val_acc: 0.2020\n",
      "Epoch 99/1000\n",
      "2708/2708 [==============================] - 0s 171us/step - loss: 0.1019 - acc: 0.2143 - val_loss: 0.3601 - val_acc: 0.1860\n",
      "Epoch 100/1000\n",
      "2708/2708 [==============================] - 0s 175us/step - loss: 0.1019 - acc: 0.2214 - val_loss: 0.3600 - val_acc: 0.1840\n",
      "Epoch 101/1000\n",
      "2708/2708 [==============================] - 0s 172us/step - loss: 0.1021 - acc: 0.1357 - val_loss: 0.3600 - val_acc: 0.1820\n",
      "Epoch 102/1000\n",
      "2708/2708 [==============================] - 0s 172us/step - loss: 0.1018 - acc: 0.2000 - val_loss: 0.3599 - val_acc: 0.2140\n",
      "Epoch 103/1000\n",
      "2708/2708 [==============================] - 0s 172us/step - loss: 0.1017 - acc: 0.2071 - val_loss: 0.3599 - val_acc: 0.2540\n",
      "Epoch 104/1000\n",
      "2708/2708 [==============================] - 0s 173us/step - loss: 0.1018 - acc: 0.2143 - val_loss: 0.3599 - val_acc: 0.3120\n",
      "Epoch 105/1000\n",
      "2708/2708 [==============================] - 0s 170us/step - loss: 0.1018 - acc: 0.1643 - val_loss: 0.3598 - val_acc: 0.3320\n",
      "Epoch 106/1000\n",
      "2708/2708 [==============================] - 0s 168us/step - loss: 0.1017 - acc: 0.1714 - val_loss: 0.3597 - val_acc: 0.3540\n",
      "Epoch 107/1000\n",
      "2708/2708 [==============================] - 0s 167us/step - loss: 0.1016 - acc: 0.2500 - val_loss: 0.3597 - val_acc: 0.3760\n",
      "Epoch 108/1000\n",
      "2708/2708 [==============================] - 0s 166us/step - loss: 0.1017 - acc: 0.1714 - val_loss: 0.3596 - val_acc: 0.5460\n",
      "Epoch 109/1000\n",
      "2708/2708 [==============================] - 0s 170us/step - loss: 0.1016 - acc: 0.2214 - val_loss: 0.3596 - val_acc: 0.5580\n",
      "Epoch 110/1000\n",
      "2708/2708 [==============================] - 0s 169us/step - loss: 0.1016 - acc: 0.2000 - val_loss: 0.3596 - val_acc: 0.4660\n",
      "Epoch 111/1000\n",
      "2708/2708 [==============================] - 0s 172us/step - loss: 0.1013 - acc: 0.2571 - val_loss: 0.3595 - val_acc: 0.4320\n",
      "Epoch 112/1000\n",
      "2708/2708 [==============================] - 0s 171us/step - loss: 0.1015 - acc: 0.2357 - val_loss: 0.3596 - val_acc: 0.4420\n",
      "Epoch 113/1000\n",
      "2708/2708 [==============================] - 0s 170us/step - loss: 0.1016 - acc: 0.1429 - val_loss: 0.3596 - val_acc: 0.4740\n",
      "Epoch 114/1000\n",
      "2708/2708 [==============================] - 0s 172us/step - loss: 0.1016 - acc: 0.2000 - val_loss: 0.3596 - val_acc: 0.3960\n",
      "Epoch 115/1000\n",
      "2708/2708 [==============================] - 0s 169us/step - loss: 0.1014 - acc: 0.2857 - val_loss: 0.3596 - val_acc: 0.3080\n",
      "Epoch 116/1000\n",
      "2708/2708 [==============================] - 0s 167us/step - loss: 0.1013 - acc: 0.2357 - val_loss: 0.3596 - val_acc: 0.1860\n",
      "Epoch 117/1000\n",
      "2708/2708 [==============================] - 0s 167us/step - loss: 0.1015 - acc: 0.1857 - val_loss: 0.3597 - val_acc: 0.1360\n",
      "Epoch 118/1000\n",
      "2708/2708 [==============================] - 0s 166us/step - loss: 0.1012 - acc: 0.2357 - val_loss: 0.3597 - val_acc: 0.1480\n",
      "Epoch 119/1000\n",
      "2708/2708 [==============================] - 0s 166us/step - loss: 0.1014 - acc: 0.1643 - val_loss: 0.3597 - val_acc: 0.1560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/1000\n",
      "2708/2708 [==============================] - 0s 173us/step - loss: 0.1015 - acc: 0.1786 - val_loss: 0.3597 - val_acc: 0.1840\n",
      "Epoch 121/1000\n",
      "2708/2708 [==============================] - 0s 171us/step - loss: 0.1016 - acc: 0.1571 - val_loss: 0.3598 - val_acc: 0.1840\n",
      "Epoch 122/1000\n",
      "2708/2708 [==============================] - 0s 172us/step - loss: 0.1015 - acc: 0.2357 - val_loss: 0.3598 - val_acc: 0.1900\n",
      "Epoch 123/1000\n",
      "2708/2708 [==============================] - 0s 170us/step - loss: 0.1015 - acc: 0.1500 - val_loss: 0.3598 - val_acc: 0.2020\n",
      "Epoch 124/1000\n",
      "2708/2708 [==============================] - 0s 171us/step - loss: 0.1014 - acc: 0.1857 - val_loss: 0.3599 - val_acc: 0.2100\n",
      "Epoch 125/1000\n",
      "2708/2708 [==============================] - 0s 170us/step - loss: 0.1012 - acc: 0.2071 - val_loss: 0.3598 - val_acc: 0.2560\n",
      "Epoch 126/1000\n",
      "2708/2708 [==============================] - 0s 169us/step - loss: 0.1013 - acc: 0.2214 - val_loss: 0.3597 - val_acc: 0.3180\n",
      "Epoch 127/1000\n",
      "2708/2708 [==============================] - 0s 167us/step - loss: 0.1015 - acc: 0.1643 - val_loss: 0.3596 - val_acc: 0.2860\n",
      "Epoch 128/1000\n",
      "2708/2708 [==============================] - 0s 167us/step - loss: 0.1013 - acc: 0.1429 - val_loss: 0.3595 - val_acc: 0.2880\n",
      "Epoch 129/1000\n",
      "2708/2708 [==============================] - 0s 167us/step - loss: 0.1013 - acc: 0.2071 - val_loss: 0.3594 - val_acc: 0.3440\n",
      "Epoch 130/1000\n",
      "2708/2708 [==============================] - 0s 167us/step - loss: 0.1011 - acc: 0.2071 - val_loss: 0.3593 - val_acc: 0.3380\n",
      "Epoch 131/1000\n",
      "2708/2708 [==============================] - 0s 169us/step - loss: 0.1012 - acc: 0.1786 - val_loss: 0.3592 - val_acc: 0.2660\n",
      "Epoch 132/1000\n",
      "2708/2708 [==============================] - 0s 170us/step - loss: 0.1013 - acc: 0.2143 - val_loss: 0.3592 - val_acc: 0.2740\n",
      "Epoch 133/1000\n",
      "2708/2708 [==============================] - 0s 170us/step - loss: 0.1011 - acc: 0.2143 - val_loss: 0.3592 - val_acc: 0.2580\n",
      "Epoch 134/1000\n",
      "2708/2708 [==============================] - 0s 172us/step - loss: 0.1010 - acc: 0.2214 - val_loss: 0.3592 - val_acc: 0.3000\n",
      "Epoch 135/1000\n",
      "2708/2708 [==============================] - 0s 170us/step - loss: 0.1011 - acc: 0.2000 - val_loss: 0.3592 - val_acc: 0.3040\n",
      "Epoch 136/1000\n",
      "2708/2708 [==============================] - 0s 172us/step - loss: 0.1013 - acc: 0.1786 - val_loss: 0.3593 - val_acc: 0.3020\n",
      "Epoch 137/1000\n",
      "2708/2708 [==============================] - 0s 168us/step - loss: 0.1013 - acc: 0.2000 - val_loss: 0.3594 - val_acc: 0.3440\n",
      "Epoch 138/1000\n",
      "2708/2708 [==============================] - 0s 169us/step - loss: 0.1013 - acc: 0.1643 - val_loss: 0.3594 - val_acc: 0.3220\n",
      "Epoch 139/1000\n",
      "2708/2708 [==============================] - 0s 168us/step - loss: 0.1010 - acc: 0.2214 - val_loss: 0.3595 - val_acc: 0.2680\n",
      "Epoch 140/1000\n",
      "2708/2708 [==============================] - 0s 170us/step - loss: 0.1011 - acc: 0.2429 - val_loss: 0.3595 - val_acc: 0.2660\n",
      "Epoch 141/1000\n",
      "2708/2708 [==============================] - 0s 171us/step - loss: 0.1013 - acc: 0.1643 - val_loss: 0.3595 - val_acc: 0.2680\n",
      "Epoch 142/1000\n",
      "2708/2708 [==============================] - 0s 173us/step - loss: 0.1008 - acc: 0.2214 - val_loss: 0.3596 - val_acc: 0.2600\n",
      "Epoch 143/1000\n",
      "2708/2708 [==============================] - 0s 173us/step - loss: 0.1010 - acc: 0.1643 - val_loss: 0.3595 - val_acc: 0.2520\n",
      "Epoch 144/1000\n",
      "2708/2708 [==============================] - 0s 174us/step - loss: 0.1013 - acc: 0.1143 - val_loss: 0.3595 - val_acc: 0.2620\n",
      "Epoch 145/1000\n",
      "2708/2708 [==============================] - 0s 177us/step - loss: 0.1013 - acc: 0.1857 - val_loss: 0.3595 - val_acc: 0.2700\n",
      "Epoch 146/1000\n",
      "2708/2708 [==============================] - 0s 174us/step - loss: 0.1010 - acc: 0.2357 - val_loss: 0.3595 - val_acc: 0.4720\n",
      "Epoch 147/1000\n",
      "2708/2708 [==============================] - 0s 172us/step - loss: 0.1009 - acc: 0.2071 - val_loss: 0.3594 - val_acc: 0.4540\n",
      "Epoch 148/1000\n",
      "2708/2708 [==============================] - 0s 170us/step - loss: 0.1011 - acc: 0.1857 - val_loss: 0.3594 - val_acc: 0.4040\n",
      "Epoch 149/1000\n",
      "2708/2708 [==============================] - 0s 171us/step - loss: 0.1012 - acc: 0.1500 - val_loss: 0.3594 - val_acc: 0.2960\n",
      "Epoch 150/1000\n",
      "2708/2708 [==============================] - 0s 167us/step - loss: 0.1011 - acc: 0.1786 - val_loss: 0.3594 - val_acc: 0.2840\n",
      "Epoch 151/1000\n",
      "2708/2708 [==============================] - 0s 166us/step - loss: 0.1011 - acc: 0.1500 - val_loss: 0.3594 - val_acc: 0.2720\n",
      "Epoch 152/1000\n",
      "2708/2708 [==============================] - 0s 167us/step - loss: 0.1011 - acc: 0.1786 - val_loss: 0.3594 - val_acc: 0.2960\n",
      "Epoch 153/1000\n",
      "2708/2708 [==============================] - 0s 170us/step - loss: 0.1010 - acc: 0.1643 - val_loss: 0.3594 - val_acc: 0.2280\n",
      "Epoch 154/1000\n",
      "2708/2708 [==============================] - 0s 170us/step - loss: 0.1011 - acc: 0.1786 - val_loss: 0.3595 - val_acc: 0.1960\n",
      "Epoch 155/1000\n",
      "2708/2708 [==============================] - 0s 170us/step - loss: 0.1009 - acc: 0.1929 - val_loss: 0.3596 - val_acc: 0.1900\n",
      "Epoch 156/1000\n",
      "2708/2708 [==============================] - 0s 170us/step - loss: 0.1012 - acc: 0.1643 - val_loss: 0.3596 - val_acc: 0.1700\n",
      "Epoch 157/1000\n",
      "2708/2708 [==============================] - 0s 171us/step - loss: 0.1010 - acc: 0.1786 - val_loss: 0.3597 - val_acc: 0.1460\n",
      "Epoch 158/1000\n",
      "2708/2708 [==============================] - 0s 170us/step - loss: 0.1012 - acc: 0.1643 - val_loss: 0.3597 - val_acc: 0.1060\n",
      "Epoch 159/1000\n",
      "2708/2708 [==============================] - 0s 168us/step - loss: 0.1009 - acc: 0.2071 - val_loss: 0.3598 - val_acc: 0.0900\n",
      "Epoch 160/1000\n",
      "2708/2708 [==============================] - 0s 167us/step - loss: 0.1011 - acc: 0.1571 - val_loss: 0.3598 - val_acc: 0.1600\n",
      "Epoch 161/1000\n",
      "2708/2708 [==============================] - 0s 166us/step - loss: 0.1012 - acc: 0.1286 - val_loss: 0.3597 - val_acc: 0.1520\n",
      "Epoch 162/1000\n",
      "2708/2708 [==============================] - 0s 166us/step - loss: 0.1009 - acc: 0.1857 - val_loss: 0.3596 - val_acc: 0.1280\n",
      "Epoch 163/1000\n",
      "2708/2708 [==============================] - 0s 171us/step - loss: 0.1010 - acc: 0.2143 - val_loss: 0.3596 - val_acc: 0.1220\n",
      "Epoch 164/1000\n",
      "2708/2708 [==============================] - 0s 172us/step - loss: 0.1010 - acc: 0.1571 - val_loss: 0.3594 - val_acc: 0.1220\n",
      "Epoch 165/1000\n",
      "2708/2708 [==============================] - 0s 176us/step - loss: 0.1010 - acc: 0.1357 - val_loss: 0.3593 - val_acc: 0.1840\n",
      "Epoch 166/1000\n",
      "2708/2708 [==============================] - 0s 173us/step - loss: 0.1011 - acc: 0.1714 - val_loss: 0.3592 - val_acc: 0.2160\n",
      "Epoch 167/1000\n",
      "2708/2708 [==============================] - 0s 173us/step - loss: 0.1011 - acc: 0.1500 - val_loss: 0.3591 - val_acc: 0.1640\n",
      "Epoch 168/1000\n",
      "2708/2708 [==============================] - 0s 170us/step - loss: 0.1009 - acc: 0.1929 - val_loss: 0.3590 - val_acc: 0.1640\n",
      "Epoch 169/1000\n",
      "2708/2708 [==============================] - 0s 168us/step - loss: 0.1008 - acc: 0.2643 - val_loss: 0.3590 - val_acc: 0.1760\n",
      "Epoch 170/1000\n",
      "2708/2708 [==============================] - 0s 166us/step - loss: 0.1010 - acc: 0.1500 - val_loss: 0.3590 - val_acc: 0.1940\n",
      "Epoch 171/1000\n",
      "2708/2708 [==============================] - 0s 166us/step - loss: 0.1010 - acc: 0.2071 - val_loss: 0.3590 - val_acc: 0.2120\n",
      "Epoch 172/1000\n",
      "2708/2708 [==============================] - 0s 165us/step - loss: 0.1008 - acc: 0.1857 - val_loss: 0.3590 - val_acc: 0.2720\n",
      "Epoch 173/1000\n",
      "2708/2708 [==============================] - 0s 166us/step - loss: 0.1011 - acc: 0.1929 - val_loss: 0.3591 - val_acc: 0.3080\n",
      "Epoch 174/1000\n",
      "2708/2708 [==============================] - 0s 169us/step - loss: 0.1010 - acc: 0.1929 - val_loss: 0.3592 - val_acc: 0.3080\n",
      "Epoch 175/1000\n",
      "2708/2708 [==============================] - 0s 171us/step - loss: 0.1009 - acc: 0.1786 - val_loss: 0.3593 - val_acc: 0.3060\n",
      "Epoch 176/1000\n",
      "2708/2708 [==============================] - 0s 169us/step - loss: 0.1011 - acc: 0.2143 - val_loss: 0.3593 - val_acc: 0.3580\n",
      "Epoch 177/1000\n",
      "2708/2708 [==============================] - 0s 171us/step - loss: 0.1009 - acc: 0.1786 - val_loss: 0.3594 - val_acc: 0.2380\n",
      "Epoch 178/1000\n",
      "2708/2708 [==============================] - 0s 172us/step - loss: 0.1009 - acc: 0.1857 - val_loss: 0.3594 - val_acc: 0.2180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 179/1000\n",
      "2708/2708 [==============================] - 0s 172us/step - loss: 0.1008 - acc: 0.2000 - val_loss: 0.3594 - val_acc: 0.1940\n",
      "Epoch 180/1000\n",
      "2708/2708 [==============================] - 0s 169us/step - loss: 0.1008 - acc: 0.2214 - val_loss: 0.3594 - val_acc: 0.1820\n",
      "Epoch 181/1000\n",
      "2708/2708 [==============================] - 0s 166us/step - loss: 0.1009 - acc: 0.1857 - val_loss: 0.3594 - val_acc: 0.1880\n",
      "Epoch 182/1000\n",
      "2708/2708 [==============================] - 0s 165us/step - loss: 0.1008 - acc: 0.1857 - val_loss: 0.3594 - val_acc: 0.2060\n",
      "Epoch 183/1000\n",
      "2708/2708 [==============================] - 0s 167us/step - loss: 0.1009 - acc: 0.2000 - val_loss: 0.3594 - val_acc: 0.2200\n",
      "Epoch 184/1000\n",
      "2708/2708 [==============================] - 0s 166us/step - loss: 0.1010 - acc: 0.1929 - val_loss: 0.3593 - val_acc: 0.2280\n",
      "Epoch 185/1000\n",
      "2708/2708 [==============================] - 0s 168us/step - loss: 0.1010 - acc: 0.1571 - val_loss: 0.3594 - val_acc: 0.2340\n",
      "Epoch 186/1000\n",
      "2708/2708 [==============================] - 0s 169us/step - loss: 0.1007 - acc: 0.1786 - val_loss: 0.3594 - val_acc: 0.2680\n",
      "Epoch 187/1000\n",
      "2708/2708 [==============================] - 0s 170us/step - loss: 0.1010 - acc: 0.1571 - val_loss: 0.3594 - val_acc: 0.2600\n",
      "Epoch 188/1000\n",
      "2708/2708 [==============================] - 0s 171us/step - loss: 0.1009 - acc: 0.1286 - val_loss: 0.3594 - val_acc: 0.2480\n",
      "Epoch 189/1000\n",
      "2708/2708 [==============================] - 0s 172us/step - loss: 0.1008 - acc: 0.2286 - val_loss: 0.3594 - val_acc: 0.2400\n",
      "Epoch 190/1000\n",
      "2708/2708 [==============================] - 0s 172us/step - loss: 0.1009 - acc: 0.1929 - val_loss: 0.3594 - val_acc: 0.2480\n",
      "Epoch 191/1000\n",
      "2708/2708 [==============================] - 0s 169us/step - loss: 0.1007 - acc: 0.2071 - val_loss: 0.3593 - val_acc: 0.2600\n",
      "Epoch 192/1000\n",
      "2708/2708 [==============================] - 0s 166us/step - loss: 0.1011 - acc: 0.1571 - val_loss: 0.3592 - val_acc: 0.2700\n",
      "Epoch 193/1000\n",
      "2708/2708 [==============================] - 0s 166us/step - loss: 0.1009 - acc: 0.1429 - val_loss: 0.3592 - val_acc: 0.3180\n",
      "Epoch 194/1000\n",
      "2708/2708 [==============================] - 0s 167us/step - loss: 0.1012 - acc: 0.0857 - val_loss: 0.3591 - val_acc: 0.2720\n",
      "Epoch 195/1000\n",
      "2708/2708 [==============================] - 0s 166us/step - loss: 0.1007 - acc: 0.1714 - val_loss: 0.3591 - val_acc: 0.2820\n",
      "Epoch 196/1000\n",
      "2708/2708 [==============================] - 0s 169us/step - loss: 0.1009 - acc: 0.1429 - val_loss: 0.3591 - val_acc: 0.2960\n",
      "Epoch 197/1000\n",
      "2708/2708 [==============================] - 0s 170us/step - loss: 0.1009 - acc: 0.1357 - val_loss: 0.3592 - val_acc: 0.2240\n",
      "Epoch 198/1000\n",
      "2708/2708 [==============================] - 0s 171us/step - loss: 0.1011 - acc: 0.1500 - val_loss: 0.3592 - val_acc: 0.2000\n",
      "Epoch 199/1000\n",
      "2708/2708 [==============================] - 0s 172us/step - loss: 0.1008 - acc: 0.1857 - val_loss: 0.3593 - val_acc: 0.1920\n",
      "Epoch 200/1000\n",
      "2708/2708 [==============================] - 0s 171us/step - loss: 0.1008 - acc: 0.1786 - val_loss: 0.3593 - val_acc: 0.1800\n",
      "Epoch 201/1000\n",
      "2708/2708 [==============================] - 0s 170us/step - loss: 0.1008 - acc: 0.2000 - val_loss: 0.3594 - val_acc: 0.1680\n",
      "Epoch 202/1000\n",
      "2708/2708 [==============================] - 0s 168us/step - loss: 0.1010 - acc: 0.1500 - val_loss: 0.3594 - val_acc: 0.1300\n",
      "Epoch 203/1000\n",
      "2708/2708 [==============================] - 0s 166us/step - loss: 0.1009 - acc: 0.1714 - val_loss: 0.3594 - val_acc: 0.1340\n",
      "Epoch 204/1000\n",
      "2708/2708 [==============================] - 0s 167us/step - loss: 0.1009 - acc: 0.1643 - val_loss: 0.3594 - val_acc: 0.0900\n",
      "Epoch 205/1000\n",
      "2708/2708 [==============================] - 0s 168us/step - loss: 0.1008 - acc: 0.1929 - val_loss: 0.3594 - val_acc: 0.1020\n",
      "Epoch 206/1000\n",
      "2708/2708 [==============================] - 0s 167us/step - loss: 0.1009 - acc: 0.1214 - val_loss: 0.3594 - val_acc: 0.1220\n",
      "Epoch 207/1000\n",
      "2708/2708 [==============================] - 0s 172us/step - loss: 0.1007 - acc: 0.1929 - val_loss: 0.3593 - val_acc: 0.2120\n",
      "Epoch 208/1000\n",
      "2708/2708 [==============================] - 0s 170us/step - loss: 0.1007 - acc: 0.2143 - val_loss: 0.3593 - val_acc: 0.3820\n",
      "Epoch 209/1000\n",
      "2708/2708 [==============================] - 0s 170us/step - loss: 0.1010 - acc: 0.1643 - val_loss: 0.3593 - val_acc: 0.2720\n",
      "Epoch 210/1000\n",
      "2708/2708 [==============================] - 0s 172us/step - loss: 0.1008 - acc: 0.1286 - val_loss: 0.3592 - val_acc: 0.1880\n",
      "Epoch 211/1000\n",
      "2708/2708 [==============================] - 0s 170us/step - loss: 0.1008 - acc: 0.1714 - val_loss: 0.3593 - val_acc: 0.1700\n",
      "Epoch 212/1000\n",
      "2708/2708 [==============================] - 0s 171us/step - loss: 0.1007 - acc: 0.2071 - val_loss: 0.3593 - val_acc: 0.1680\n",
      "Epoch 213/1000\n",
      "2708/2708 [==============================] - 0s 169us/step - loss: 0.1009 - acc: 0.1643 - val_loss: 0.3593 - val_acc: 0.1980\n",
      "Epoch 214/1000\n",
      "2708/2708 [==============================] - 0s 167us/step - loss: 0.1008 - acc: 0.2071 - val_loss: 0.3594 - val_acc: 0.2120\n",
      "Epoch 215/1000\n",
      "2708/2708 [==============================] - 0s 168us/step - loss: 0.1007 - acc: 0.1857 - val_loss: 0.3595 - val_acc: 0.2280\n",
      "Epoch 216/1000\n",
      "2708/2708 [==============================] - 0s 165us/step - loss: 0.1007 - acc: 0.2429 - val_loss: 0.3595 - val_acc: 0.2340\n",
      "Epoch 217/1000\n",
      "2708/2708 [==============================] - 0s 162us/step - loss: 0.1009 - acc: 0.1357 - val_loss: 0.3595 - val_acc: 0.2640\n",
      "Epoch 218/1000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-38371aa40545>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m           \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m           callbacks=[es_callback,mc_callback])\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;31m# callbacks=[es_callback,tb_callback,mc_callback]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# Test model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\learn software\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32mD:\\learn software\\anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\learn software\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3790\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3791\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3792\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3793\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3794\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\learn software\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1603\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1604\u001b[0m     \"\"\"\n\u001b[1;32m-> 1605\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1606\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1607\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\learn software\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1643\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1644\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1645\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1647\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\learn software\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1746\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\learn software\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    599\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mD:\\learn software\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "validation_data = ([X,A],Y_val,idx_val)\n",
    "model.fit([X,A],\n",
    "         Y_train,\n",
    "        sample_weight=idx_train,\n",
    "          epochs=epochs,\n",
    "          batch_size=N,\n",
    "          validation_data=validation_data,\n",
    "          shuffle=False,\n",
    "          callbacks=[es_callback,mc_callback])\n",
    "# callbacks=[es_callback,tb_callback,mc_callback]\n",
    "# Test model\n",
    "eval_results = model.evaluate([X,A],\n",
    "                             Y_test,\n",
    "                             sample_weight=idx_test,\n",
    "                             batch_size=N,\n",
    "                             verbose=0)\n",
    "\n",
    "print('Done.\\n'\n",
    "      'Test loss: {}\\n'\n",
    "      'Test accuracy: {}'.format(*eval_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
